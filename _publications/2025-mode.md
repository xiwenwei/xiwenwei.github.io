---
title: "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models"
collection: publications
category: conferences
permalink: /publication/2025-mode
date: 2025-12-05
venue: 'The Thirty-ninth Annual Conference on Neural Information Processing Systems'
paperurl: 'https://www.arxiv.org/abs/2512.03125'
citation: '<strong>Xiwen Wei</strong>, Mustafa Munir, & Radu Marculescu. (2025). &quot;Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models.&quot; <i>The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS)</i>.'
---

<!-- @inproceedings{
wei2025mitigating,
title={Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models},
author={Xiwen Wei and Mustafa Munir and Radu Marculescu},
booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},
year={2025},
url={https://openreview.net/forum?id=CBsANtjBV4}
} -->

[![paper](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://www.arxiv.org/abs/2512.03125)

Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings. Our code is available [here](https://github.com/Christina200/MoDE-official). 
